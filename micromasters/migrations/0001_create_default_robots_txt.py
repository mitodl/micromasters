# Generated by Django 2.2.28 on 2023-01-08 23:39

from urllib.parse import urlparse

from django.conf import settings
from django.db import migrations

# A list containing all the existing disallowed URLS that were part of legacy robots.txt.
EXISTING_DISALLOWED_URLS = ["/dashboard/", "/learner/", "/learners/", "/cms/", "/profile/", "/financial_aid/", "/admin/", "/logout/",
                            "/pearson/", "/api/", "/certificate/", "/records/", "/letter/"]


def create_default_robots_txt(apps, schema_editor):
    Site = apps.get_model("sites", "Site")
    Url = apps.get_model("robots", "Url")
    Rule = apps.get_model("robots", "Rule")

    domain = urlparse(settings.SITE_BASE_URL).netloc

    # django.contrib.sites should be creating this, but it's in a delayed post-migration hook:
    #
    current_site, created = Site.objects.get_or_create(
        pk=getattr(settings, "SITE_ID", 1), defaults=dict(domain=domain, name=domain)
    )

    rule, created = Rule.objects.get_or_create(robot="*")
    rule.sites.add(current_site)

    # Pre populate the robots.txt with existing disallowed URLs
    for disallowed_url in EXISTING_DISALLOWED_URLS:
        url, _ = Url.objects.get_or_create(pattern=disallowed_url)
        rule.disallowed.add(url)
        rule.save()


class Migration(migrations.Migration):

    dependencies = [("sites", "0002_alter_domain_unique"), ("robots", "0001_initial")]

    operations = [
        migrations.RunPython(create_default_robots_txt, migrations.RunPython.noop)
    ]
